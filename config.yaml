module: fl
model_name: transformers
  # - textcnn
models:
  - transformers:
    - vocab_size: 98635
    - num_classes: 10
    - embadding_dim: 32
    - token_max_len: 200
    - head_num: 8
    - nlayers: 2
clients: 10
learning_rate: 0.001
dataset: IMDB
device: gpu
join_ratio: 0.5
algorithm:
  -Ditto:
    -mu: 0.2
    -per_local_steps: 2